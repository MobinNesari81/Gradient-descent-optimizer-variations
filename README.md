# Gradient-descent-optimizer-variations
This repository contains implementation of stochastic gradient descent, SGD with momentum, Adagrad, RMSprop, Adam optimzerÂ  from scratch using Python language.
